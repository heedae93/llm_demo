import torch
import torch.nn as nn
import torch.optim as optim

# 1ï¸âƒ£ ë°ì´í„° ì¤€ë¹„
corpus = ["ë‚˜ëŠ” ì‚¬ê³¼ë¥¼ ë¨¹ì—ˆê³  ë”¸ê¸°ë„ ë¨¹ì—ˆì§€ë§Œ ì˜¤ë Œì§€ë„ ë¨¹ê³  ì‹¶ë‹¤"]
tokens = corpus[0].split()
vocab = sorted(set(tokens))
word2idx = {w: i for i, w in enumerate(vocab)}
idx2word = {i: w for w, i in word2idx.items()}

# 2ï¸âƒ£ (ì…ë ¥, ì •ë‹µ) ìŒ ë§Œë“¤ê¸°
data = []
for i in range(len(tokens) - 1):
    x = torch.tensor([[word2idx[tokens[i]]]])  # [batch=1, seq_len=1]
    y = torch.tensor([word2idx[tokens[i + 1]]])  # [batch=1]
    data.append((x, y))

# í•™ìŠµ ë°ì´í„° í™•ì¸ ì¶œë ¥
print("ğŸ“¦ í•™ìŠµ ë°ì´í„°ì…‹ (ì…ë ¥ â†’ ì •ë‹µ):")
for i, (x, y) in enumerate(data):
    print(f"{i+1}) {idx2word[x.item()]} â†’ {idx2word[y.item()]}")

# 3ï¸âƒ£ Transformer ì–¸ì–´ëª¨ë¸ ì •ì˜
class TransformerLM(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc = nn.Linear(d_model, vocab_size)

    def forward(self, x):  # x: [batch, seq_len]
        x = self.embed(x)            # [batch, seq_len, d_model]
        x = self.transformer(x)      # [batch, seq_len, d_model]
        out = self.fc(x[:, -1, :])   # ë§ˆì§€ë§‰ ìœ„ì¹˜ ì¶œë ¥ë§Œ ì‚¬ìš© â†’ [batch, vocab_size]
        return out

# 4ï¸âƒ£ ëª¨ë¸ ì´ˆê¸°í™”
vocab_size = len(vocab)
model = TransformerLM(vocab_size, d_model=16, nhead=2, num_layers=1)
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# ì´ íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸
total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"\nğŸ”¢ ëª¨ë¸ ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params}")

# 5ï¸âƒ£ í•™ìŠµ ì „ ê°€ì¤‘ì¹˜ ì¼ë¶€ í™•ì¸
print("\nğŸ§Š í•™ìŠµ ì „ fc.weight ì¼ë¶€:")
print(model.fc.weight[:3])

# 6ï¸âƒ£ í•™ìŠµ ë£¨í”„
print("\nğŸ” í•™ìŠµ ì‹œì‘")
for epoch in range(10):
    total_loss = 0
    print(f"\nğŸŒ€ Epoch {epoch + 1}")
    for i, (x, y) in enumerate(data):
        optimizer.zero_grad()
        pred = model(x)            # [1, vocab_size]
        loss = loss_fn(pred, y)    # [1, vocab_size] vs [1]
        loss.backward()

        # ê°€ì¤‘ì¹˜ ë³€í™” ì¶”ì 
        target_word_idx = 0
        target_word = idx2word[target_word_idx]
        weight_before = model.fc.weight.data.clone()[target_word_idx][:3]

        optimizer.step()
        weight_after = model.fc.weight.data.clone()[target_word_idx][:3]

        print(f"  ğŸ“¥ ì…ë ¥: {idx2word[x.item()]} ({x.item()})")
        print(f"  ğŸ¯ ì •ë‹µ: {idx2word[y.item()]} ({y.item()})")
        print(f"  ğŸ§  ì˜ˆì¸¡: {idx2word[pred.argmax().item()]} ({pred.argmax().item()})")
        print(f"  âŒ ì†ì‹¤: {loss.item():.6f}")
        print(f"  ğŸ§Š fc.weight['{target_word}'] (ì „): {weight_before}")
        print(f"  ğŸ”¥ fc.weight['{target_word}'] (í›„): {weight_after}\n")

        total_loss += loss.item()

    print(f"âœ… Epoch {epoch + 1} ì¢…ë£Œ, í‰ê·  ì†ì‹¤: {total_loss / len(data):.4f}")

# 7ï¸âƒ£ í•™ìŠµ í›„ ê°€ì¤‘ì¹˜ ì¼ë¶€ ì¶œë ¥
print("\nâœ… í•™ìŠµ ì™„ë£Œ! fc.weight ì¼ë¶€:")
print(model.fc.weight[:3])

# 8ï¸âƒ£ ì˜ˆì¸¡ í•¨ìˆ˜
def predict(word):
    if word not in word2idx:
        print(f"âŒ ë‹¨ì–´ '{word}'ëŠ” ë‹¨ì–´ì¥ì— ì—†ìŠµë‹ˆë‹¤.")
        return
    with torch.no_grad():
        x = torch.tensor([[word2idx[word]]])  # [batch=1, seq_len=1]
        out = model(x)
        pred_idx = torch.argmax(out, dim=1).item()
        print(f"ğŸ”® ì˜ˆì¸¡: '{word}' â†’ '{idx2word[pred_idx]}'")

# 9ï¸âƒ£ ì˜ˆì¸¡ ì‹¤í–‰
print("\nğŸ” ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸:")
predict("ë‚˜ëŠ”")
predict("ì‚¬ê³¼ë¥¼")
predict("ë¨¹ì—ˆê³ ")
predict("ë”¸ê¸°ë„")
predict("ë¨¹ì—ˆì§€ë§Œ")
predict("ì˜¤ë Œì§€ë„")
predict("ë¨¹ê³ ")
predict("ì‹¶ë‹¤")