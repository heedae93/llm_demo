import torch
import torch.nn as nn
import torch.optim as optim

# 1ï¸âƒ£ í•™ìŠµ ë°ì´í„° ì •ì˜
corpus = ["ë‚˜ëŠ” ë‚˜ëŠ” ì‚¬ê³¼ë¥¼ ë¨¹ì—ˆë‹¤"]

# 2ï¸âƒ£ ë‹¨ì–´ ì‚¬ì „ ìƒì„± , ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•´ì•¼ í•œë‹¤.
token_list = list(set(" ".join(corpus).split())) # ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ ë¶„ë¦¬ í•˜ê³  ì¤‘ë³µ ë‹¨ì–´ ì œê±°
token_list.sort() # ì‚¬ì „ìˆœ ì •ë ¬
word2idx = {w: i for i, w in enumerate(token_list)}
idx2word = {i: w for w, i in word2idx.items()} # ë‹¨ì–´ë¥¼ keyë¡œ í•˜ê³  ê°’ì´ indexì¸ ë”•ì…”ë„ˆë¦¬ ìƒì„±
print("ë‹¨ì–´ ì¸ë±ìŠ¤ ë§¤í•‘:", word2idx)

# 3ï¸âƒ£ í›ˆë ¨ ë°ì´í„° ìƒì„± (ì…ë ¥ ë‹¨ì–´ â†’ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡) , ì…ë ¥ê³¼ ì •ë‹µ ìŒ ( x, y ) ë¥¼ ë§Œë“œëŠ” ê³¼ì •
data = []
tokens = corpus[0].split()
for i in range(len(tokens) - 1):
    x = torch.tensor([word2idx[tokens[i]]])    # ì…ë ¥ ë‹¨ì–´
    y = torch.tensor([word2idx[tokens[i + 1]]])  # ì •ë‹µ ë‹¨ì–´
    data.append((x, y))
print("\ní•™ìŠµ ìƒ˜í”Œ:")



for x, y in data:
    print(f"ì…ë ¥: {idx2word[x.item()]} â†’ ì •ë‹µ: {idx2word[y.item()]}")



# 4ï¸âƒ£ ì•„ì£¼ ê°„ë‹¨í•œ LSTM ì–¸ì–´ëª¨ë¸ ì •ì˜
class MiniLSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim)
        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x):
        x = self.embed(x)  # ë‹¨ì–´ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        x, _ = self.lstm(x.view(len(x), 1, -1))  # LSTM ì²˜ë¦¬
        out = self.fc(x[-1])  # ë§ˆì§€ë§‰ ì‹œê°„ì˜ ì¶œë ¥ â†’ ë¶„ë¥˜
        return out

# 5ï¸âƒ£ ëª¨ë¸, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì„¤ì •
vocab_size = len(word2idx)
model = MiniLSTM(vocab_size, embed_dim=10, hidden_dim=20)
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)

# 6ï¸âƒ£ í•™ìŠµ ì „ ì´ˆê¸° ê°€ì¤‘ì¹˜ ì¼ë¶€ ì¶œë ¥
print("\nâœ… ì´ˆê¸° ê°€ì¤‘ì¹˜ ìƒ˜í”Œ (fc.weight):")
print(model.fc.weight[:3])  # ì¼ë¶€ ì¶œë ¥

# 7ï¸âƒ£ í•™ìŠµ ë£¨í”„
print("\nğŸ” í•™ìŠµ ì‹œì‘")
for epoch in range(10):
    total_loss = 0
    for x, y in data:
        optimizer.zero_grad()
        pred = model(x)
        loss = loss_fn(pred, y)
        loss.backward()     # ğŸ”§ ì—­ì „íŒŒ: ì†ì‹¤ ê¸°ë°˜ íŒŒë¼ë¯¸í„° ë³€í™”ëŸ‰ ê³„ì‚°
        optimizer.step()    # ğŸ’¡ íŒŒë¼ë¯¸í„° ê°±ì‹  (ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸)
        total_loss += loss.item()
    print(f"Epoch {epoch+1}, Loss: {total_loss:.4f}")

# 8ï¸âƒ£ í•™ìŠµ í›„ ê°€ì¤‘ì¹˜ ì¶œë ¥
print("\nâœ… í•™ìŠµ í›„ ê°€ì¤‘ì¹˜ ìƒ˜í”Œ (fc.weight):")
print(model.fc.weight[:3])  # ì¼ë¶€ ì¶œë ¥

# 9ï¸âƒ£ ì˜ˆì¸¡ í™•ì¸
def predict(word):
    if word not in word2idx:
        print(f"ë‹¨ì–´ '{word}'ëŠ” ì‚¬ì „ì— ì—†ìŠµë‹ˆë‹¤.")
        return
    with torch.no_grad():
        x = torch.tensor([word2idx[word]])
        out = model(x)
        predicted_idx = torch.argmax(out).item()
        print(f"{word} â†’ {idx2word[predicted_idx]}")

print("\nğŸ”® ì˜ˆì¸¡ ê²°ê³¼:")
predict("ë‚˜ëŠ”")
predict("ì‚¬ê³¼ë¥¼")
