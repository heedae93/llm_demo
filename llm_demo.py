import torch
import torch.nn as nn
import torch.optim as optim


# í•™ìŠµì´ë€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ìˆœì„œë¥¼ í†µê³„ì ìœ¼ë¡œ ê¸°ì–µí•˜ê³  ì¼ë°˜í™” í•˜ëŠ” ê³¼ì •



# 1ï¸âƒ£ í•™ìŠµ ë°ì´í„° ì •ì˜
corpus = ["ë‚˜ëŠ” ì‚¬ê³¼ë¥¼ ë¨¹ì—ˆë‹¤"]

# 2ï¸âƒ£ ë‹¨ì–´ ì‚¬ì „ ìƒì„± ( í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•´ì•¼ í•œë‹¤. ì™œëƒí•˜ë©´ ë‹¤ìŒ ë‹¨ê³„ì¸ ì„ë² ë”© ê³¼ì •ì—ì„œ ìˆ«ì ì¸ë±ìŠ¤ë§Œ ì…ë ¥ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ )
token_list = list(set(" ".join(corpus).split()))
token_list.sort()
# ê° ë‹¨ì–´ì— ì¸ë±ìŠ¤ ì§€ì •
word2idx = {w: i for i, w in enumerate(token_list)}
#  # ë‹¨ì–´ë¥¼ keyë¡œ í•˜ê³  ê°’ì´ indexì¸ ë”•ì…”ë„ˆë¦¬ ìƒì„± ( ì´ê±¸ í•˜ëŠ” ì´ìœ ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ê°€ ìˆ«ì ì¸ë±ìŠ¤ë¡œ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— ê·¸ê±¸ ë‹¤ì‹œ ë‹¨ì–´ë¡œ ëŒë¦¬ê¸° ìœ„í•¨ )
idx2word = {i: w for w, i in word2idx.items()}
# print("ë‹¨ì–´ ì¸ë±ìŠ¤ ë§¤í•‘:", word2idx)

# 3ï¸âƒ£ í›ˆë ¨ ë°ì´í„° ìƒì„± ( ì…ë ¥ ë‹¨ì–´ -> ì •ë‹µ ë‹¨ì–´ ìŒë“¤ì˜ ì§‘í•© ) , ì—¬ê¸°ì„œ ì…ë ¥ ë‹¨ì–´ëŠ” ì´ì „ ê¸€ì ì •ë‹µ ë‹¨ì–´ëŠ” ë‹¤ìŒì— ì˜¤ëŠ” ê¸€ì
data = []
tokens = corpus[0].split()
for i in range(len(tokens) - 1):
    # ì…ë ¥ ë‹¨ì–´
    x = torch.tensor([word2idx[tokens[i]]])
    # ì •ë‹µ ë‹¨ì–´
    y = torch.tensor([word2idx[tokens[i + 1]]])
    data.append((x, y))
# tensorëŠ” PyTorchì—ì„œ ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ ë°ì´í„° êµ¬ì¡°ë¡œ, ì‰½ê²Œ ë§í•´ **ìˆ«ìë¥¼ ë‹´ëŠ” ë‹¤ì°¨ì› ë°°ì—´ ì˜ˆë¥¼ë“¤ì–´ tensor([2]) ëŠ” tensor ìë£Œêµ¬ì¡°ì— 2ë¼ëŠ” ê°’ì´ ë“¤ì–´ê°€ ìˆëŠ” ê²ƒ.


print("ğŸ“¦ í•™ìŠµ ì „ ìµœì¢… ë°ì´í„° ì…‹:")
for i, (x, y) in enumerate(data):
    print(f"{i+1}) ì…ë ¥ í…ì„œ: {x}, ì •ë‹µ í…ì„œ: {y}")

print("ğŸ“¦ dataì— ì €ì¥ëœ ì¸ë±ìŠ¤ì˜ ì‹¤ì œ ë¬¸ìì—´ ê°’ë“¤ :")
for x, y in data:
    print(f"ì…ë ¥: {idx2word[x.item()]} â†’ ì •ë‹µ: {idx2word[y.item()]}")



# 4ï¸âƒ£ ì•„ì£¼ ê°„ë‹¨í•œ LSTM ì–¸ì–´ëª¨ë¸ ì •ì˜ ( PyTorchì˜ ëª¨ë¸ ê¸°ë³¸í˜•ì¸ nn.Moduleì„ ìƒì†ë°›ì€ ì‹ ê²½ë§ í´ë˜ìŠ¤ ) ë° ì„¸íŒ…
class MiniLSTM(nn.Module):

    # ìƒì„±ì
    def __init__(self, vocab_size, embed_dim, hidden_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, embed_dim) # ì „ì²´ ë‹¨ì–´ì˜ ìˆ˜ , ì°¨ì› ì…‹íŒ…
        self.lstm = nn.LSTM(embed_dim, hidden_dim)
        self.fc = nn.Linear(hidden_dim, vocab_size)

    # ëª¨ë¸ ì‹¤í–‰ ë©”ì„œë“œ
    def forward(self, x):
        x = self.embed(x)                                   # ë‹¨ì–´ ì¸ë±ìŠ¤ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        x, _ = self.lstm(x.view(len(x), 1, -1))             # LSTM ì²˜ë¦¬
        out = self.fc(x[-1])                                # ë§ˆì§€ë§‰ ì‹œê°„ì˜ ì¶œë ¥ â†’ ë¶„ë¥˜
        return out


vocab_size = len(word2idx)
# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
model = MiniLSTM(vocab_size, embed_dim=5, hidden_dim=5)
# ì´ íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸
total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"\nğŸ“¦ ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params}")
#ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µ ë¹„êµì— ì‚¬ìš©í•  ì†ì‹¤í•¨ìˆ˜ ì„¤ì • ( ì†ì‹¤ í•¨ìˆ˜ë€ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ì •ë‹µ ì‚¬ì´ì˜ ì˜¤ì°¨ë¥¼ ìˆ˜ì¹˜ë¥´ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ , ì˜ˆì¸¡ì´ í‹€ë¦¬ë©´ ê°’ì´ ì»¤ì§€ê³  ë§ìœ¼ë©´ ê°’ì´ ì‘ì•„ì§ )
loss_fn = nn.CrossEntropyLoss()
#ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•  ì˜µí‹°ë§ˆì´ì € ì„¤ì • ( ì†ì‹¤ í•¨ìˆ˜ê°€ ê³„ì‚°í•œ ì˜¤ì°¨ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë°”ê¾¸ëŠ” ì—­í•  )
optimizer = optim.SGD(model.parameters(), lr=0.1)

# 6ï¸âƒ£ í•™ìŠµ ì „ ì´ˆê¸° ê°€ì¤‘ì¹˜ ì¼ë¶€ ì¶œë ¥
print("\nâœ… ì´ˆê¸° ê°€ì¤‘ì¹˜ ìƒ˜í”Œ (fc.weight):")
print(model.fc.weight[:3])  # ì¼ë¶€ ì¶œë ¥



# 7ï¸âƒ£ í•™ìŠµ ë£¨í”„
print("\nğŸ” í•™ìŠµ ì‹œì‘")
for epoch in range(10):                  # ğŸ” ì „ì²´ í•™ìŠµì„ 10ë²ˆ ë°˜ë³µ
    total_loss = 0
    for x, y in data:                    # ğŸ” ëª¨ë“  í›ˆë ¨ ë°ì´í„° ìƒ˜í”Œì— ëŒ€í•´
        optimizer.zero_grad()            # ğŸ”„ ì´ì „ ê³„ì‚°í•œ ê¸°ìš¸ê¸° ì´ˆê¸°í™”
        pred = model(x)                  # â›³ï¸ ëª¨ë¸ ì˜ˆì¸¡ê°’ ê³„ì‚°
        loss = loss_fn(pred, y)          # âŒ ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µ ì°¨ì´ ê³„ì‚° (ì†ì‹¤)
        loss.backward()                  # ğŸ”§ ì˜¤ì°¨ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ë³€í™”ëŸ‰ ê³„ì‚°
        optimizer.step()                 # ğŸ’¡ íŒŒë¼ë¯¸í„° ê°±ì‹  ( # ğŸ’¡ ê°€ì¤‘ì¹˜ ì‹¤ì œë¡œ ì—…ë°ì´íŠ¸!)
        total_loss += loss.item()        # ğŸ“Š ì†ì‹¤ ëˆ„ì 
    print(f"Epoch {epoch+1}, Loss: {total_loss:.4f}")

# 8ï¸âƒ£ í•™ìŠµ í›„ ê°€ì¤‘ì¹˜ ì¶œë ¥
print("\nâœ… í•™ìŠµ í›„ ê°€ì¤‘ì¹˜ ìƒ˜í”Œ (fc.weight):")
print(model.fc.weight[:3])  # ì¼ë¶€ ì¶œë ¥

# 9ï¸âƒ£ ì˜ˆì¸¡ í™•ì¸
def predict(word):
    if word not in word2idx:
        print(f"ë‹¨ì–´ '{word}'ëŠ” ì‚¬ì „ì— ì—†ìŠµë‹ˆë‹¤.")
        return
    with torch.no_grad():
        x = torch.tensor([word2idx[word]])
        out = model(x)
        predicted_idx = torch.argmax(out).item()
        print(f"{word} â†’ {idx2word[predicted_idx]}")

print("\nğŸ”® ì˜ˆì¸¡ ê²°ê³¼:")
predict("ë‚˜ëŠ”")
predict("ì‚¬ê³¼ë¥¼")
