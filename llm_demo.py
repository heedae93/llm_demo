import torch
import torch.nn as nn
import torch.optim as optim


# í•™ìŠµì´ë€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ìˆœì„œë¥¼ í†µê³„ì ìœ¼ë¡œ ê¸°ì–µí•˜ê³  ì¼ë°˜í™” í•˜ëŠ” ê³¼ì •



# 1ï¸âƒ£ í•™ìŠµ ë°ì´í„° ì •ì˜
corpus = ["ë‚˜ëŠ” ì‚¬ê³¼ë¥¼ ë¨¹ì—ˆë‹¤"]

# 2ï¸âƒ£ ë‹¨ì–´ ì‚¬ì „ ìƒì„± ( í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•´ì•¼ í•œë‹¤. ì™œëƒí•˜ë©´ ë‹¤ìŒ ë‹¨ê³„ì¸ ì„ë² ë”© ê³¼ì •ì—ì„œ ìˆ«ì ì¸ë±ìŠ¤ë§Œ ì…ë ¥ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ )
token_list = list(set(" ".join(corpus).split()))
token_list.sort()
# ê° ë‹¨ì–´ì— ì¸ë±ìŠ¤ ì§€ì •
word2idx = {w: i for i, w in enumerate(token_list)}
#  # ë‹¨ì–´ë¥¼ keyë¡œ í•˜ê³  ê°’ì´ indexì¸ ë”•ì…”ë„ˆë¦¬ ìƒì„± ( ì´ê±¸ í•˜ëŠ” ì´ìœ ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ê°€ ìˆ«ì ì¸ë±ìŠ¤ë¡œ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— ê·¸ê±¸ ë‹¤ì‹œ ë‹¨ì–´ë¡œ ëŒë¦¬ê¸° ìœ„í•¨ )
idx2word = {i: w for w, i in word2idx.items()}
# print("ë‹¨ì–´ ì¸ë±ìŠ¤ ë§¤í•‘:", word2idx)

# 3ï¸âƒ£ í›ˆë ¨ ë°ì´í„° ìƒì„± ( ì…ë ¥ ë‹¨ì–´ -> ì •ë‹µ ë‹¨ì–´ ìŒë“¤ì˜ ì§‘í•© ) , ì—¬ê¸°ì„œ ì…ë ¥ ë‹¨ì–´ëŠ” ì´ì „ ê¸€ì ì •ë‹µ ë‹¨ì–´ëŠ” ë‹¤ìŒì— ì˜¤ëŠ” ê¸€ì
# tensorëŠ” PyTorchì—ì„œ ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ ë°ì´í„° êµ¬ì¡°ë¡œ, ì‰½ê²Œ ë§í•´ **ìˆ«ìë¥¼ ë‹´ëŠ” ë‹¤ì°¨ì› ë°°ì—´ ì˜ˆë¥¼ë“¤ì–´ tensor([2]) ëŠ” tensor ìë£Œêµ¬ì¡°ì— 2ë¼ëŠ” ê°’ì´ ë“¤ì–´ê°€ ìˆëŠ” ê²ƒ.
data = []
tokens = corpus[0].split()
for i in range(len(tokens) - 1):
    # ì…ë ¥ ë‹¨ì–´
    x = torch.tensor([word2idx[tokens[i]]])
    # ì •ë‹µ ë‹¨ì–´
    y = torch.tensor([word2idx[tokens[i + 1]]])
    data.append((x, y))



print("ğŸ“¦ í•™ìŠµ ì „ ìµœì¢… ë°ì´í„° ì…‹:")
for i, (x, y) in enumerate(data):
    print(f"{i+1}) ì…ë ¥ í…ì„œ: {x}, ì •ë‹µ í…ì„œ: {y}")

print("ğŸ“¦ dataì— ì €ì¥ëœ ì¸ë±ìŠ¤ì˜ ì‹¤ì œ ë¬¸ìì—´ ê°’ë“¤ :")
for x, y in data:
    print(f"ì…ë ¥: {idx2word[x.item()]} â†’ ì •ë‹µ: {idx2word[y.item()]}")



# 4ï¸âƒ£ ì•„ì£¼ ê°„ë‹¨í•œ LSTM ì–¸ì–´ëª¨ë¸ ì •ì˜ ( PyTorchì˜ ëª¨ë¸ ê¸°ë³¸í˜•ì¸ nn.Moduleì„ ìƒì†ë°›ì€ ì‹ ê²½ë§ í´ë˜ìŠ¤ ) ë° ì„¸íŒ… , LSTMì€ ì–¸ì–´ ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•œ ì•„í‚¤í…ì³
class MiniLSTM(nn.Module):

    # ìƒì„±ì, ë©¤ë²„ ë³€ìˆ˜ì˜ ê° ê³„ì¸µì„ í• ë‹¹
    def __init__(self, vocab_size, embed_dim, hidden_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, embed_dim) # ë‹¨ì–´ë¥¼ ìˆ«ì ë²¡í„°ë¡œ ë°”ê¿”ì£¼ëŠ” ì „ì²˜ë¦¬ê¸° (ì…ë ¥ ê³„ì¸µ)
        self.lstm = nn.LSTM(embed_dim, hidden_dim) # ë‹¨ì–´ ì‚¬ì´ì˜ ë¬¸ë§¥ì„ ì´í•´í•˜ëŠ” ë‡Œ ì—­í•  (ì¤‘ê°„ ê³„ì¸µ)
        self.fc = nn.Linear(hidden_dim, vocab_size) # ìµœì¢… ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•´ì„œ ë½‘ì•„ì£¼ëŠ” ì¶œë ¥ê¸° (ì¶œë ¥ ê³„ì¸µ)

    # ëª¨ë¸ ì‹¤í–‰ ë©”ì„œë“œ , forwardí•¨ìˆ˜ì˜ ê²°ê³¼ê°€ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’
    def forward(self, x):
        x = self.embed(x)                                   # ë‹¨ì–´ ì¸ë±ìŠ¤ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
        x, _ = self.lstm(x.view(len(x), 1, -1))             # LSTM ì²˜ë¦¬
        out = self.fc(x[-1])                                # ë§ˆì§€ë§‰ ì‹œê°„ì˜ ì¶œë ¥ â†’ ë¶„ë¥˜
        return out



vocab_size = len(word2idx)
# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± , ì´ ë¶€ë¶„ì— ì…ë ¥í•œ ìˆ«ìë¡œ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ê°€ ì •í•´ì§
model = MiniLSTM(vocab_size, embed_dim=5, hidden_dim=5)
# ì´ íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸
total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"\nğŸ“¦ ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params}")
# ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µ ë¹„êµì— ì‚¬ìš©í•  ì†ì‹¤í•¨ìˆ˜ ì„¤ì • ( ì†ì‹¤ í•¨ìˆ˜ë€ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ì •ë‹µ ì‚¬ì´ì˜ ì˜¤ì°¨ë¥¼ ìˆ˜ì¹˜ë¥´ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ , ì˜ˆì¸¡ì´ í‹€ë¦¬ë©´ ê°’ì´ ì»¤ì§€ê³  ë§ìœ¼ë©´ ê°’ì´ ì‘ì•„ì§ )
loss_fn = nn.CrossEntropyLoss()
#ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•  ì˜µí‹°ë§ˆì´ì € ì„¤ì • ( ì†ì‹¤ í•¨ìˆ˜ê°€ ê³„ì‚°í•œ ì˜¤ì°¨ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë°”ê¾¸ëŠ” ì—­í•  )
optimizer = optim.SGD(model.parameters(), lr=0.1)

# 6ï¸âƒ£ í•™ìŠµ ì „ ì´ˆê¸° ê°€ì¤‘ì¹˜ ì¼ë¶€ ì¶œë ¥
print("\nâœ… ì´ˆê¸° ê°€ì¤‘ì¹˜ ìƒ˜í”Œ (fc.weight):")
print(model.fc.weight[:3])  # ì¼ë¶€ ì¶œë ¥

print("\nğŸ” í•™ìŠµ ì‹œì‘")
for epoch in range(10):  # ğŸ” ì „ì²´ í•™ìŠµ 10ë²ˆ ë°˜ë³µ
    total_loss = 0
    print(f"\nğŸŒ€ Epoch {epoch + 1}")

    for i, (x, y) in enumerate(data):  # ğŸ” í›ˆë ¨ ìƒ˜í”Œ ìˆœíšŒ
        optimizer.zero_grad()

        # 1ï¸âƒ£ ì˜ˆì¸¡
        pred = model(x)

        # 2ï¸âƒ£ ì†ì‹¤ ê³„ì‚°
        loss = loss_fn(pred, y)

        # 3ï¸âƒ£ ì—­ì „íŒŒ (ê¸°ìš¸ê¸° ê³„ì‚°)
        loss.backward()

        # ğŸ’¡ ì–´ë–¤ ë‹¨ì–´ì˜ ê°€ì¤‘ì¹˜ í™•ì¸í• ì§€ (ì˜ˆ: ë‹¨ì–´ 0ë²ˆ = idx2word[0])
        target_word_idx = 0
        target_word = idx2word[target_word_idx]
        weight_before = model.fc.weight.data.clone()[target_word_idx][:3]

        print(f"  Sample {i + 1}")
        print(f"    ğŸ“¥ ì…ë ¥ x         : {x.item()} â†’ ({idx2word[x.item()]})")
        print(f"    ğŸ¯ ì •ë‹µ y         : {y.item()} â†’ ({idx2word[y.item()]})")
        print(f"    ğŸ§  ì˜ˆì¸¡ pred      : {pred.argmax().item()} â†’ ({idx2word[pred.argmax().item()]})")
        print(f"    âŒ ì†ì‹¤(loss)     : {loss.item():.6f}")
        print(f"    ğŸ§Š fc.weight[ë‹¨ì–´ '{target_word}'] (ì „) : {weight_before}")

        # 4ï¸âƒ£ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        optimizer.step()

        weight_after = model.fc.weight.data.clone()[target_word_idx][:3]
        print(f"    ğŸ”¥ fc.weight[ë‹¨ì–´ '{target_word}'] (í›„) : {weight_after}")

        total_loss += loss.item()

    print(f"âœ… Epoch {epoch + 1} ë! í‰ê·  ì†ì‹¤: {total_loss / len(data):.4f}")


# 8ï¸âƒ£ í•™ìŠµ í›„ ê°€ì¤‘ì¹˜ ì¶œë ¥
print("\nâœ… í•™ìŠµ í›„ ê°€ì¤‘ì¹˜ ìƒ˜í”Œ (fc.weight):")
print(model.fc.weight[:3])  # ì¼ë¶€ ì¶œë ¥

# 9ï¸âƒ£ ì˜ˆì¸¡ í™•ì¸
def predict(word):
    if word not in word2idx:
        print(f"ë‹¨ì–´ '{word}'ëŠ” ì‚¬ì „ì— ì—†ìŠµë‹ˆë‹¤.")
        return
    with torch.no_grad():
        x = torch.tensor([word2idx[word]])
        out = model(x)
        predicted_idx = torch.argmax(out).item()
        print(f"{word} â†’ {idx2word[predicted_idx]}")

print("\nğŸ”® ì˜ˆì¸¡ ê²°ê³¼:")
predict("ë‚˜ëŠ”")
predict("ì‚¬ê³¼ë¥¼")
